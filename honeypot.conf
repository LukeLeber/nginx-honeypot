#
# This file is protected under the KILLGPL.
# For more information, visit http://lukeleber.github.io/KILLGPL.html
#
# This file should be included in a http context
#

# Trimmed down access log format for our honeypot visitors
log_format honeypot '$remote_addr - $remote_user [$time_local] '
                    '"$http_referer" "$http_user_agent"';

#
# we split visitors up into 4 distinct categories:
# 1. fleshy human users - these visitors can also be bots pretending to be users
# 2. (good) search bots - these bots help with SEO
# 3. neutral bots       - these bots do not help with SEO, but play by the rules
# 4. bad bots           - these bots have ignored the robots.txt file
#
map $http_user_agent $visitor_type {
    default '';
    ~*(googlebot|crawler|baiduspider|80legs|ia_archiver|voyager|curl|wget|yahoo!\sslurp|mediapartners-google) 'good';
    ~*(Huaweisymantecspider|Offline\sExplorer|SiteSnagger|TeleportPro|WebCopier|WebReaper|WebStripper|WebZIP|Xaldon_WebSpider) 'neutral';

    # the bots below have fallen into our honeypot
    #~*() 'bad';
}

#
# If the visitor is a bot, then set $is_bot to their IP.  This allows us to 
# throttle requests on a per-bot basis without hindering fleshy users.
#
map $visitor_type $is_bot {
    default     '';
    ^'bad'      $binary_remote_addr;
    ^'neutral'  $binary_remote_addr;
    ^'good'     $binary_remote_addr;
}   

#
# Enforce the desired request throttle specified in our robots.txt file
# Note - the Crawl-delay directive is non-standard and relatively ambiguous
#
limit_req_zone       $is_bot zone=bots:1m rate=1r/s;
limit_req_log_level  error;
